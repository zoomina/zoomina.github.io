---
layout: post
title:  "심리학으로 읽어보는 딥러닝 - 강화학습 DQN"
date:   2019-03-23
image:  deepPsy3.png
tags:   [Pysychology, AI, DeepLearning]
sitemap:
    changefreq: daily
    priority: 1.0
---

이번 주제에서 언급되는 이야기들은 아마 심리학이나 인공지능에 관심이 없는 분들도 한 번쯤은 들어보셨을 거예요.  
심리학으로 읽어보는 딥러닝 세 번째 이야기는 바로 강화학습입니다.  

<br>

혹시 2016년 알파고와 이세돌 9단의 대국을 기억하시나요?  
그 당시까지만 해도 바둑은 어마어마한 경우의 수에 기인하여 아직 컴퓨터가 정복하지 못한 영역이라는 의미를 갖고 있었습니다.  
그리고 딥마인드는 알파고를 통해 그 영역을 정복해내며 딥러닝 열풍을 이끌었습니다.  
  
오늘은 그 딥마인드가 개발한 알고리즘으로 유명한 DQN과 심리학 실험을 통해 강화학습에 대한 이야기를 들려드리려고 합니다.  

<br>

---

<br>

## 파블로프의 개와 고전적 조건형성

<br>

아마 많은 분들이 오늘 들려드릴 실험의 이름 혹은 그 내용이 익숙하게 느껴질거예요.  
고전적 조건형성이 무엇인지는 몰라도 '파블로프의 개'정도는 들어보셨을거라 예상합니다.  

파블로프는 러시아 출신의 생물학자였습니다.  
그는 소화 계통의 실험을 진행하던 중, 음식이 주어지기 전부터 침을 흘리는 개의 모습에 의문을 갖게 됩니다.  

<br>

꾸준한 관찰 끝에 음식을 가져다주는 조교가 들어올 때마다 개가 침을 흘린다는 사실을 발견한 파블로프는 다음의 실험을 고안하였습니다.  

<br>

<img src="https://user-images.githubusercontent.com/39390943/149232212-07dfdf30-a9d6-4f28-b73c-042c19b7d302.png" width=500>

<br>

파블로프는 개의 침샘에서 분비되는 침의 양을 측정할 수 있는 실험장치를 준비했습니다.  
그리고 음식과 종소리를 이용해 개에게 자극을 주고 반응을 기록하였습니다.  

이때 자극은 3가지로 분류됩니다.  

> 1. 무조건 자극(Unconditioned stimuli) : 학습과 무관하게 반응을 일으키는 자극
> 2. 중립 자극(Neutral stimuli) : 반응과 관계없는 자극
> 3. 조건 자극(Conditioned stimuli) : 학습을 통해 반응을 일으킬 수 있는 자극

<br>

조건 형성이 되기 전까지 개는 음식이 주어졌을 때에만 침이 분비되었고, 종소리에는 반응하지 않았습니다.  
여기서 음식은 무조건적으로 반응을 일으키는 무조건 자극에 해당하고, 종소리는 반응과 관계없는 중립 자극에 해당합니다.  
  
이후 종소리를 조건 자극으로 설정하고, 음식을 주기 전에 항상 종을 울렸습니다.  
  
두 자극 간에 충분한 연관이 생길 무렵, 음식을 주지 않고 종소리를 들려주었습니다.  
이때 개는 음식이 없음에도 침을 분비하였습니다!  

<br>

파블로프는 이와 같이 두 자극이 서로 연결되어 새로운 반응을 학습하는 과정을 조건형성이라고 정의하였습니다.  

<br>

---

<br>

## 스키너 상자와 조작적 조건형성

<br>

파블로프가 두 자극의 연관을 통한 학습을 조건형성이라고 정의하였지만, 그 앞엔 '고전적'이라는 단어가 붙었습니다.  
바로 조건형성에 대한 추가적인 연구가 진행되었기 때문입니다.  

<br>

이후의 학자들은 인간이 외부 자극에 기계적으로 반응하는 것이 아니라, 환경을 스스로 조작하면서 반응한다고 생각했습니다.  
  
이야기를 더 진행시키기 전에 이해를 돕기 위한 영상을 먼저 보여드릴게요.  
이 영상은 비둘기에게 Turn이라는 글자를 가르치는 영상입니다!  

<br>

<iframe width="804" height="603" src="https://www.youtube.com/embed/TtfQlkGwE2U" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<br>

사실 스키너는 쥐 실험으로 더 유명한데요, 개인적으로 이 실험에서 점진적인 행동의 변화가 두드러지기 때문에 참고용으로 더 적합하다고 판단하였습니다. :blush:  

<br>

영상을 보시면 처음엔 먹이가 나오는 곳에 주목을 시켰습니다.  
배고픈 비둘기는 먹이를 더 먹고싶어 두리번거리다가 고개를 살짝 돌렸습니다.  
이 행동은 우리가 원하는 Turn에 가깝기 때문에 먹이를 줍니다.  
고개를 돌리면 먹이가 나온다는 사실을 인지한 비둘기는 더 자주 고개를 돌리네요.  
그럼 이제 절반정도 돌아갔을 때 먹이를 줍니다.  
이런 식으로 비둘기의 행동은 점차 Turn에 가까워지더니 결국 Turn을 학습하게 되었습니다.  

<br>

영상 속의 비둘기와 같이 의식적으로 어떤 결과를 일으키도록 조작하는 행동을 조작적 행동이라 부르며, 이러한 행동을 통해 형성되는 조건형성을 조작적 조건형성이라고 합니다.  
  
조작적 조건화에서 가장 핵심이 되는 개념은 바로 **강화**와 **처벌**입니다.  
  
위의 영상에서 강화물은 먹이인데요, 이는 선호 자극을 제공하여 행동을 강화한 것으로 볼 수 있습니다.  
다만 강화는 보상이 주어지는 것 뿐만 아니라, 혐오 자극을 제거해주는 것을 포함하여 특정 행동의 빈도수를 높이기 위해 자극을 활용하는 모든 과정입니다.  

<br>

예를 들어 쓰레기를 줍는 학생에게 칭찬(선호 자극 제공)을 했습니다.  
칭찬을 듣고 기분이 좋아진 학생에게 쓰레기를 줍는 행동은 강화될 것입니다.  
하지만 지저분한 방에서 쓰레기를 주워서(혐오 자극 제거) 방이 깨끗해졌고, 깨끗한 방을 보며 뿌듯함을 느꼈다면 마찬가지로 쓰레기를 줍는 행동은 강화될 수 있겠지요.  

<br>

반대로 처벌은 행동의 빈도수를 낮추기 위해 자극을 활용하는 모든 과정을 의미합니다.  
마찬가지로 혐오 자극을 제공하는 것도 처벌이지만, 선호 자극을 제거하는 것 또한 처벌이 될 수 있습니다.  
  
게임을 너무 많이 하는 아이에게 화를 내는 것(혐오 자극 제공)이 게임 시간을 줄일 수도 있지만, 용돈을 줄이는 것(선호 자극 제거) 또한 게임 시간을 줄일 수 있습니다.  

<br>

---

<br>

## 인공지능의 강화학습

<br>

그럼 이번에는 인공지능에서 강화학습 모델은 어떤 구조인지 살펴볼까요?  
스키너 상자 이야기를 떠올려보면 아마 대략적인 구조에 대한 느낌이 오실거라고 생각합니다.  

<br>

서론에서 언급했듯이, 이번 이야기에서 다룰 모델은 DQN입니다.  
DQN은 Deep Q-network의 줄임말로, 강화학습 알고리즘으로 유명한 Q-learning을 딥러닝으로 구현한 모델입니다.  

<br>

Q-learning은 행동에 대한 가치를 예측하는 Q 함수를 학습하는 알고리즘으로, 그 구조는 아래와 같습니다.  

<img src="https://user-images.githubusercontent.com/39390943/149237906-c7445735-e0bc-4d97-8867-ba6786fd0812.png" width=300>

<br>

여기서 Agent는 Environment의 state에서 학습을 수행하며 의사결정을 내리는 주체, 즉 영상 속의 비둘기에 해당합니다.  
Environment는 점점 올라가는 보상 기준과 먹이가 주어지는 상자가 되겠지요?  

상자 속의 비둘기가 먹이를 먹을 수 있는 방향으로 학습한 것처럼, Q-learning 속의 Agent 또한 reward의 총합을 가장 크게 만드는 방향으로 학습을 수행합니다.  

<br>

다만 Q-learning을 그대로 딥러닝으로 옮기기에는 두 가지의 문제점이 존재했습니다.  
  
1. 선택된 데이터에 따라 결과가 편향될 수 있기 때문에 학습이 불안정해질 수 있다.  

> <img src="https://user-images.githubusercontent.com/39390943/149241236-efd0cb03-12e5-4d81-a79a-e0e8fc15d86c.png">

> 첫 번째 그래프는 전체 데이터를 나타내고 있습니다.  
> 그 안에서 3개의 데이터를 선택하여 학습한 결과가 2, 3번째 그래프에 해당합니다.  
> 여기서 두 번째 그래프처럼 선택된 데이터가 치우쳐져 있을 경우 전체 데이터를 반영하지 못하기 때문에 제대로 학습이 이루어지지 못하고 있음을 확인할 수 있습니다.  

<br>

2. 목표값이 움직이기 때문에 학습이 불안정해질 수 있다.

> Q-learning은 Q 함수를 학습하는 알고리즘이기 때문에 Q 함수에 의존성을 갖게 됩니다.  
> 이때, Q 함수가 업데이트되면 목표값이 움직이기 때문에 이로 인한 불안정함이 유발될 수 있습니다.  

<br>

딥마인드에서는 이러한 문제점을 해결하고 아래와 같은 구조로 DQN을 완성했습니다.  

<img src="https://user-images.githubusercontent.com/39390943/149242236-defb9d21-8700-4160-a32e-00cdb7429457.jpg" width=300>

<br>

1번 문제에 대한 해결책으로 Agent가 수행한 모든 데이터를 시간별로 저장했습니다.  
그리고 그 안에서 랜덤으로 하나의 state를 선택하여 학습을 진행합니다.  
이렇게 학습을 진행하면 특수한 상황에 치우치는 경우가 줄어들게 되며, 과거의 상태에 대해 반복적으로 학습이 이루어지는 효과도 있습니다.  
  
2번 문제를 해결하기 위해서는 기본 신경망과 목표 신경망을 분리하여 고정된 목표값을 설정해주었습니다.  
기본 신경망에서는 Agent가 학습을 진행하며 최적의 행동을 얻어냅니다.  
그리고 일정 주기마다 기본 신경망의 결과를 바탕으로 목표 신경망에서 목표값을 갱신합니다.  

<br>

이 뿐만이 아니라 영상을 이용하여 게임 환경에 대해 학습합니다.  
여기에서 우리는 CNN을 이용했으리라고 짐작해볼 수 있겠지요?  

<br>

---

<br>

이번 주제에 대해 작성하며 처음 DQN을 접했을 때 작성한 발표자료를 다시 읽어보았습니다.  
대략적인 개념은 이해했지만, 방대하고 낯선 개념과 수식에 압도당해 포기한 흔적이 보이네요.  
지금은 그 때보다 더 많이 이해할 수 있지만, 이번 연재글의 목표는 '최대한 수식 없이 표현하자!'이기 때문에 과감하게 모두 지워버렸습니다.  
하지만 '난 이걸로 부족해!'라고 느끼시는 분들은 참고자료 중 가장 마지막에 있는 슬라이드를 참고하시면 좋을 것 같아요!  

<br>

익숙한 심리학 실험을 통해 만나본 세 번째 주제는 어떠셨나요?  
저는 다음 글에서 마지막 주제인 언어 모델을 가지고 다시 찾아오겠습니다.  

<br>

---

<br>

> ### 참고자료
> E.Bruce Goldstein, 감각과 지각 7판, cengage learning  
> 사이토 고키, 밑바닥부터 시작하는 딥러닝, 한빛미디어  
> 김진중, 골빈해커의 3분 딥러닝, 한빛미디어  
> 김진우, 강화학습 기초부터 DQN까지, slideshare(https://www.slideshare.net/CurtPark1/dqn-reinforcement-learning-from-basics-to-dqn)