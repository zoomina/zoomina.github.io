---
layout: post
title:  "심리학으로 읽어보는 딥러닝 - 뉴런과 퍼셉트론"
date:   2018-12-13
image:  deepPsy1.jpg
tags:   [Pysychology, AI, DeepLearning]
sitemap:
    changefreq: daily
    priority: 1.0
---

## 심리학으로 읽어보는 딥러닝
---

안녕하세요. 심리학으로 읽어보는 딥러닝을 연재하게 된 민아입니다.  
많은 분들이 공학에 심리학은 조금 뜬금없는 조합이라고 생각하실 것 같습니다.  
이번 글에서는 딥러닝에 심리학을 엮게 된 간단한 이유와 함께 딥러닝의 시작이 되는 퍼셉트론에 대한 이야기를 풀어보려고 합니다.  
개발 1년차 병아리 심리학과 학부생이 풀어내는 이야기라 전문성이 많이 떨어지는 교양 수준의 내용이 되겠지만, 이 연재글이 입문자 여러분들께는 심적 부담감을 줄여주고 머신러닝에 익숙하신 분들께는 새로운 시각을 열어주는 계기가 될 수 있다면 좋겠습니다.  

<br>

저희 학교 인지심리학 교수님은 심리학 뿐만 아니라 공학에도 큰 관심을 갖고 종종 공대 교수님들과 협업하시는 분입니다.  
그리고 교수님께서 수업 중에 공학에 대한 관심을 독려하시며 이런 내용의 이야기를 해주셨습니다.  

```
기계는 그냥 생겨나는 것이 아니라 자연에 있는 무언가를 모방해 발전한다.
그리고 아직까지는 진화적으로 가장 성공적인 개체는 인간이라고 할 수 있다.
그렇다면 컴퓨터는 인간을 닮아가는 방향으로 발전할 것이다.
그렇기에 여러분이 심리학에 갇혀있지 않고 공학적인 이슈에도 관심을 가졌으면 좋겠다.
너희는 공학자들이 할 수 없는 생각을 할 수 있는 사람들이다.
```

평소 인지심리와 뇌과학에 관심을 가지고 있었던 저는, 교수님의 영향으로 자연스레 인공지능에 관심을 갖게 되었습니다.  
그리고 공부를 하는 과정에서 정말 많은 알고리즘이 심리학과 수업에서 배운 내용과 닮아있다는 생각이 들었습니다.  

<br>

그럼 지금부터 평범한 심리학과 학부생이 딥러닝 알고리즘을 어떻게 바라보는지 소개해드릴게요!  


<br>

## 딥러닝의 시작, 퍼셉트론
---

<br>

이번 시간에는 가볍게 '퍼셉트론'이라는 개념에 대한 이야기를 풀어보고자 합니다.

<br>

퍼셉트론은 Frank Rosenblatt가 1967년에 고안한 알고리즘으로, 신경망 알고리즘(딥러닝)의 기원이 되는 개념입니다.  
첫 시간부터 이 개념을 가져온 이유는 기본적인 개념일 뿐만 아니라 단적으로 인공지능과 심리학의 연관관계를 보여줄 수 있는 이슈이기 때문입니다.  
퍼셉트론을 고안한 Rosenblatt은 사실 심리학자였습니다.  
이를 설명하는 용어 중 다수가 심리학의 용어와 같은 것은 아마 이러한 이유 때문이지 않을까 싶습니다.  

<br>

![neuron](https://user-images.githubusercontent.com/39390943/49379438-7826fc80-f752-11e8-8e98-d8f4ccd3f63a.png)

<br>

인간의 뇌는 **뉴런**이라는 단위로 이루어져 있습니다.  
아마 (중고등학생때라도) 생명과학을 배운 사람들은 한 번쯤 들어보았을 그 이름입니다.  
다만 여기서는 뉴런의 자세한 구조에 대해서는 살피지 않을 예정입니다.  
우리가 짚고 넘어가야 하는 것은 뉴런의 활성화 방식이 **All-or-None**이라는 점입니다.  

<br>

All-or-None 방식은 이름을 듣고 여러분이 바로 떠올리셨을 그 내용입니다.  
뉴런은 입력된 신호가 역치<sup>[1)](#footnote_1)</sup>보다 작으면 활성화되지 않습니다.  
역치를 넘어선 자극에 대해서도 마찬가지로 그 강도와 상관없이 **하나**의 뉴런이 활성화됩니다.  
즉, 우리가 느끼는 강도는 활성화된 뉴런의 개수나 빈도로 결정이 되며, 그 강도가 아무리 강하다고 하더라도 개별 뉴런은 일정하게 반응합니다.  

> <a name="footnote_1">1)</a> 역치 : 생물이 외부환경의 변화, 즉 자극에 대해 어떤 반응을 일으키는 데 필요한 최소한의 자극의 세기이다. [두산백과]

<br>

<img src="https://user-images.githubusercontent.com/39390943/49334680-da7ee080-f61e-11e8-8e70-456085a0ae9a.png" width="500">

<br>


위의 그래프를 통해 이 방식에 대해 설명을 해보려고 합니다.  
뉴런이 발화하는 기준은 -55mV로, 이 기준을 지금부터 역치라고 부르겠습니다.  
우선, Week로 표시된 부분에서는 역치를 넘기지 못했기 때문에 뉴런이 발화하지 않았습니다.  
그 뒤의 Good enough와 Super strong으로 표시된 두 부분은 역치를 넘겼기 때문에 뉴런이 발화하는 모습을 확인할 수 있습니다.  
하지만 두 자극 모두 같은 모양으로 활성화된 것을 확인할 수 있습니다.  
이를 통해 뉴런은 자극의 강도를 나타내지 않는다는 사실을 확인할 수 있습니다.  

<br>

<img src="https://user-images.githubusercontent.com/39390943/49334705-9809d380-f61f-11e8-9478-717b816d50b9.jpeg" width="500">

퍼셉트론에서도 이와 비슷한 일이 일어납니다.  
앞서 설명한 뉴런의 발화를 컴퓨터에 옮겨 0과 1로 표현하고, 역치를 임계값으로 바꾸어 설명할 수 있는데요, 위의 그림을 통해 자세히 살펴보겠습니다.  
<br>
그림에서는 4개의 입력값에 가중치(weight)를 곱하고 모두 합한 값이 Activation function으로 계단함수를 만나고 있습니다.  
이때, 계단함수는 임계값(보통 0.5)을 넘지 못하는 값에 대해 0을, 넘는 값에 대해 1을 출력하는 함수이기 때문에 뉴런과 같은 기능을 하고 있다는 사실을 확인할 수 있습니다.  

<br>

그리고 우리는 가중치와 임계값을 조정해 퍼셉트론의 기능을 조정할 수 있는데요,  
이를 수식으로 나타내자면 아래와 같이 표현할 수 있습니다.  

![formula](https://user-images.githubusercontent.com/39390943/49374597-32176c00-f745-11e8-9a2a-8ada3b3f9c20.png)

<br>

여기에서 $w$는 가중치 weight를, $\theta$는 임계값을 나타냅니다.  

<br>


이를 쉽게 설명하기 위해 '할머니 세포'와 비교해보겠습니다.  

<br>

![grandma_cell](https://user-images.githubusercontent.com/39390943/49380336-c9d08680-f754-11e8-9952-e92e97043dcb.png)

할머니 세포는 할머니에게만 반응하는 세포가 있다는 주장으로, 뇌에서 신경세포가 부호화되는 방법 중 '특정신경 부호화'라는 개념을 뒷받침하기 위한 대표적인 예시 입니다.  
즉, 하나의 퍼셉트론이 한 가지의 판단을 할 수 있도록 최적화시켜 할머니 세포와 같이 작동할 수 있도록 만들 수 있다는 것이지요.  


<br>

하지만, 퍼셉트론에는 치명적인 한계가 있었습니다.  
바로 선형적인 경계만을 표현할 수 있다는 점인데요, 이는 평면에서 하나의 직선으로 나눌 수 있는 문제만 해결할 수 있다는 의미입니다.  
그리고 현실세계의 문제는 그렇게 간단하지만은 않지요.  

<br>

<img src="https://user-images.githubusercontent.com/39390943/49379907-b7098200-f753-11e8-9a4f-f67ccfb7f3de.png" width="500">

아주 간단하고 대표적인 예시로는 배타적 논리합인 XOR 기능이 있습니다.  
이렇듯 매우 간단한 기능조차 수행할 수 없는 것이 드러난 퍼셉트론은 끝을 맞이하는 듯 보였습니다.  

<br>

----

<br>

## 퍼셉트론에서 신경망으로

<br>

심리학에서 뇌에 대한 연구는 아래와 같은 순서로 발전했습니다.  
즉, 현대 심리학은 뉴런 자체보다는 그 신경망의 구조에 집중하고 있습니다.  

<br>

![synapse_flow](https://user-images.githubusercontent.com/39390943/49199952-e596fe00-f3dd-11e8-8852-43d7cf42a331.png)

<br>

앞서 살펴보았던 퍼셉트론에도 다층 구조를 적용하면 인간의 신경망을 구현할 수 있지 않았을까요?  
안타깝게도 그 당시에는 퍼셉트론을 다층으로 쌓아올릴 수가 없었습니다.  
하지만 1986년 샌디에이고 캘리포니아 대학의 David Rumelhart와 Geoffrey Hinton을 비롯한 여러 심리학자들과 컴퓨터 관련 학자들의 신경망에 대한 연구를 토대로 연구의 판도가 바뀌게 되었습니다.  
이들이 고안한 역전파 알고리즘을 이용하여 퍼셉트론을 다층으로 쌓아올릴 수 있게 된 것이지요.  

<br>

역전파 알고리즘의 영향으로 등장한 다층 퍼셉트론(Multi Layer Perceptron, MLP)은 XOR을 포함하여 지금까지 해결할 수 없던 많은 문제를 해결하는 데에 공헌합니다.  
수십년만에 다시 신경망 연구에 활기가 돌기 시작한 것입니다!  

<br>

역전파 알고리즘은 말 그대로 전파가 역으로 전달되며 가중치를 수정하는 것입니다.  
인간의 뇌에서도 이와 비슷한 역할을 하는 세포가 있습니다.  

<br>

<img src="https://user-images.githubusercontent.com/39390943/148814463-27b928dc-0ed5-42d3-948e-a9ce46e31768.png" width="300">

<br>

인간의 뇌에는 뉴런 외에도 뉴런을 돕는 교세포(glia)가 다수 자리하고 있습니다.  
그 중 별모양의 Astrocytes라는 세포는 시냅스에 남아있는 화학물질을 흡수하여 다시 돌려주는 역할을 하고 있는데요,  
이를 통해 불필요한 뉴런들을 정리하는 역할을 하고 있습니다.  

<br>

역전파 역시 마찬가지로 Astrocytes처럼 신경망 학습에 도움을 주는 요소입니다.  
앞서 살펴보았던 내용 중 가중치(weights)라는 개념이 기억 나시나요?  
역전파는 이 가중치를 조정해 신경망 학습에 도움을 줍니다.  

<br>

신경망 학습은 크게 4개의 단계에 거쳐 진행이 됩니다.(여기서 신경망은 인공 신경망을 의미합니다.)  


> **1단계 - 미니배치**  
> 훈련데이터 중 일부를 무작위로 가져옵니다. 이렇게 선별한 데이터를 미니배치라 하며, 그 니배치의 손실함수 값을 줄이는 것이 목표입니다.  
>
> **2단계 - 기울기 산출**  
> 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구합니다. 기울기는 손실함수의 값을 가장 작게 하는 방향을 제시합니다.  
>
> **3단계 - 매개변수 갱신**  
> 가중치 매개변수를 기울기 방향으로 아주 조금 갱신합니다.  
>
> **4단계 - 반복**


<br>

그리고 역전파가 이용되는 것은 바로 2단계입니다.  
2단계에서 기울기를 구할 때 미분을 효율적으로 계산할 수 있도록 돕는 것이 바로 역전파의 역할입니다.

<br>

---

<br>

뇌의 세포들을 통해 퍼셉트론에 대해 알아보는 오늘의 이야기는 여기서 마무리를 지어보려고 합니다.  
어설픈 내용이지만, 두 학문의 유사성을 통해 제가 느낀 흥미를 다른 누군가와 함께 공유할 수 있는 기회가 된다면 정말 기쁠 것 같습니다.  

다음 글에서는 인간의 시각을 통해 CNN 알고리즘에 대한 이야기를 들려드리려고 합니다.  
긴 글 읽어주셔서 감사드리고, 다음 이야기도 기대해주세요!  

<br>

----

> ### 참고자료  
> E.Bruce Goldstein, 감각과 지각 7판, cengage learning  
> Jamse W. Kalat, 생물심리학 13판, cengage learning  
> 사이토 고키, 밑바닥부터 시작하는 딥러닝, 한빛미디어  
> 페드로 도밍고스, 마스터 알고리즘, 비즈니스북스  
> 이대열, 지능의 탄생, 바다출판사  
> 정원석, 뇌에서 astrocyte와 microglia의 phagocytic activity에 대한 연구, 분자세포생물학 뉴스레터  
