---
layout: post
title: "[2020-1학기] 인지과학 응용의 대표적 사례에 대한 개인의 생각에 대한 에세이"
date: 2020-09-09
image:  cognitive_science1.png
tags: [Pysychology, cognitive_science]
sitemap:
    changefreq: daily
    priority: 1.0
---

## 뉴로모픽 칩 Loihi : 하드웨어 성능 향상에 따른 인공지능 성능 개선

<br>

CES 2020, AMD에서 7나노 공정의 프로세서를 공개<sup>[[1]](#footnote_1)</sup>하며 이목을 끌었지만, 여전히 인텔은 10나노+(14nm)에 머무른 채 뜬금없이 AI 기술에 대한 포부를 밝히며<sup>[[2]](#footnote_2)</sup> 사람들을 당황하게 만들었다. CPU의 표준과도 같은 인텔이 후발주자인 AMD에 따라잡히고서 하고 싶은 말이 그것 뿐이라니! 하지만 얼마 뒤 이런 기사를 접하고 앞선 발표를 다시금 떠올릴 수밖에 없었다. “코 없는 컴퓨터 칩이 냄새를 구별한다 ‘뉴로모픽 컴퓨팅’”<sup>[[3]](#footnote_3)</sup> 인텔의 뉴로모픽 칩 Loihi에 대한 기사였다.  
뉴로모픽 칩<sup>[[4]](#footnote_4)</sup>은 인간의 시냅스 구조에 착안하여 뇌의 구조 및 특성을 알고리즘 뿐만 아니라 회로에서도 구현하고자 등장했다. 인간과 비슷한 수준의 연산을 컴퓨터 알고리즘으로 이끌어내려면 인간에 비해 굉장히 큰 에너지가 필요하게 되는데, 하드웨어의 구조를 개선하여 이 격차를 줄이고 보다 고차적인 지능을 모방하기 위해 해당 분야가 연구되고 있다. 인간의 시냅스에서는 ‘전기적인’ 신호가 오고간다. 뉴로모픽 칩은 이에 착안하여 각 코어들이 스파이크 신호를 주고받아 정보를 처리하도록 설계 한다. 그리고 Loihi<sup>[[5]](#footnote_5)</sup>의 도입부에서는 이를 Artificial Neural Network(ANN)과 비교하여 Spiking Neural Network(SNN)이라 칭하였다.  

<br>

SNN은 인간의 뇌를 모방한 만큼 기본적인 처리요소로 뉴런을 갖는다. 그리고 하나 이상의 뉴런의 연결을 시냅스로 지칭하고 그 사이에는 단비트 임펄스인 스파이크(spike)를 주고받는다. 뉴런은 상태 변수가 임계값을 초과하면 발화하는데, 수학적으로는 시냅스 반응 전류 와 막 전위라는 두 가지의 내부 상태 변수가 잘 알려져 있는 CUBA leaky-integrate-and-fire 모델의 변형을 채택하였다. Loihi는 디지털 아키텍처로, 고정 크기의 이산 시간 단계의 모델을 사용하면서 이를 연속 시간에 근사시켰다.  
SNN에서의 연산은 뉴런 상태의 상호작용을 통해 수행되며, 이는 기존의 선형 대수 기반의 접근법과 상당히 다른 특성을 보인다.(선형 대수 기반의 접근은 병렬 연산에 대한 유용성을 바탕으로 중요하게 여겨지고 있다.) 해당 논문에서는 대표적인 사례로 LASSO로 알려진 L1-minimizing sparse coding problem을 들었다. 이는 선형 회귀 모델에서 나타나는 overfitting 문제를 해결하기 위해 weight들의 절대값의 합이 최소가 되도록 한다는 제약조건을 추가한 문제이다. SNN에서는 적절한 네트워크를 구성한다면 네트워크의 역동에 따라서 뉴런의 평균 스파이크 속도가 고정된 지점으로 수렴하고, 그 고정점이 최적화 문제의 해결책이 된다. 이는 기존에 cross-validation을 이용해 최적값을 설정해주던 것과는 다른 양상이다.  
Loihi의 아키텍처는 128개의 뉴로모픽 코어와 4개의 평면 방향으로 인접 코어에 대해 계층적으로 확장하는 멀티코어 메쉬 등으로 이루어져있다. 각 뉴로모픽 코어는 뉴런을 구성하는 트리 세트로 묶인 1024개의 원시 스파이크 신경단위를 구현한다. 또한 메쉬 프로토콜은 온칩 코어 및 계층적 주소 지정을 통해 메시지의 확장을 지원한다. 아래의 그림 1은 SNN 모델을 실행할 때 뉴로모픽 메쉬의 작동을 보여준다.  

![image](https://user-images.githubusercontent.com/39390943/92552051-ae37cf80-f29a-11ea-8a7d-12a2902fae66.png)

첫 번째 그림은 시간 t에 대한 초기 유휴상태로, 각 사각형은 여러개의 뉴런을 포함하는 메쉬의 코어이다. 두 번째 그림에서 코어 A, B의 뉴런 n_1, n_2 는 발화하여 스파이크를 생성한다. 세 번째로 코어 A, B의 시간 단계 t에서 발화하는 다른 모든 뉴런들로부터의 스파이크는 목표 코어로 분산된다. 시간 단계의 마지막에는 모든 스파이크가 전달되었는지, t+1로 진행하는 것이 안전한지 확인이 필요한데, 여기서는 clock이 아닌 장벽 동기화 매커니즘을 사용한다. 마지막 그림에서 각 코어는 시간 t를 마치며 인접 코어와 handshaking을 한다. 장벽 메시지는 떠있는 모든 스파이크를 flush하고, 2단계로 모든 코어에 시간 단계 사전 통지를 전한다. 2단계 장벽 메시지를 받은 코어는 시간을 t+1로 업데이트한다.  

![image](https://user-images.githubusercontent.com/39390943/92552072-bdb71880-f29a-11ea-838f-07754d2a8143.png)

그림 2는 Loihi 뉴로모픽 코어의 내부 구조를 보여준다. 다이어그램 속의 각 블록은 코어에 매핑된 모든 뉴런의 연결성, 구성성, 동적 상태를 저장하는 주요 메모리를 나타낸다. 입출력 axon(N_axin, N_axout), 시냅스 메모리 크기(N_sync), 총 뉴런 칸의 수(N_cx)는 네트워크 연결 제약을 가한다.(이는 SNN에서 지역화를 위해 강조하는 부분이다.) 처리량 병목 현상의 균형을 맞추기 위해서는 코어의 파이프라인 섹션에 다양한 수준의 병렬화와 직렬화가 적용된다. 점선으로 그려진 화살표는 잠재적으로 단일 사건이 많은 종속 사건으로 확장되는 설계의 일부를 나타내며, 이런 영역에서는 하드웨어를 병렬로 사용한다.  
생물학적인 신경망은 근본적으로 비동기적이고, 이러한 특징은 SNN 모델에 명시적인 동기화(clock)가 없는 것을 통해 반영된다. Fine-grain 흐름 제어는 비동기 디자인의 중요한 속성으로, SNN에 여러 이점을 제공한다. SNN의 활동은 공간과 시간 전체에서 희박하기 때문에 지속적으로 흐르는 clock에 의해 낭비되는 전력을 제거한다. 또한 지역적 흐름 제어는 서로 다른 모듈이 자연적인 마이크로아키텍처의 주파수에 작동할 수 있게 하여 백엔드 타이밍 폐쇄를 단순화 할 수 있다. 마지막으로 타이밍 마진을 줄이거나 제거하여 불필요한 메쉬 유휴 시간을 제거함으로써 전역적인 수행상의 이점을 제공할 수 있다.  

<br>

도입부에 소개한 기사의 주인공인 냄새 맡는 칩은 Loihi를 이용한 회로인 Pohoiki Springs<sup>[[7]](#footnote_7)</sup><sup>[[8]](#footnote_8)</sup>를 지칭하는 것이다. 아쉽게도 관련 논문을 찾지 못해 기사들의 내용을 통해 살펴보면, 이 는 총 768개의 Loihi칩으로 이루어진 회로이다. IntelNahuku 보드는 Loihi칩 32개로 구성된 회로인데, 이를 각 3개씩 8행으로 구성한 형태가 바로 Pohoiki Springs이다. 하지만 그 크기에 비해 소비 전력은 500와트 정도로 게이밍 PC와 비슷한 정도에 그친다. 특히 기사들에서 주요하게 다루고 있던 포인트는 인텔과 코넬 대학의 공동 연구로 진행되고 있는 냄새를 인식하는 실험에 관한 것이었다.  
연구원들은 풍동을 통해 10개의 기체 상태에 대한 데이터 셋을 만들었는데, 이때 사용된 센서는 72개의 화학 감지 센서였다. 각 센서가 Loihi로 데이터를 전송하고, 두뇌 회로를 모방하여 학습이 일어난다. Loihi는 학습을 통해 위험한 10개의 화학 물질(아세톤, 암모니아, 메탄 등)을 냄새로 인식할 수 있게 되었다. 또한 예측되지 못한 혼입이 있더라도 타깃 물질을 정확하게 탐지해냈다. 이러한 시스템과 흔하게 접할 수 있는 화학적 감지기와의 차별점은 학습된 화학물질에 대한 지능적 분류를 가능케 했다는 점이다. 이는 유해물 탐지, 품질 관리 및 질병 탐지에서도 유용하게 사용될 수 있을 것으로 전망되고 있다.  
또한 이 회로는 오직 후각 시스템의 모방을 위해서 설계된 것이 아니라 하나의 가능성으로서 후각 시스템을 선택한 것으로, 다른 분야로의 확장 또한 충분히 기대해볼 수 있다. 우리는 시청각 시스템을 모방하는 기기를 일상적으로 이용하고 있다. 하지만 후각 시스템을 모방하는 시도로 크게 성공한 사례를 접하지 못했는데, 이는 시청각 시스템에 비해 후각 시스템은 복잡한 차원의 문제를 다루고 있기 때문일 것이다. Pohoiki Springs의 사례는 단순히 후각 시스템을 모방하기 위한 시도가 아니라 보다 복잡한 차원의 문제로의 접근이 가능해졌다는 점을 시사하는 것이다. 이는 알파고가 단순히 바둑 챔피언들을 이긴 것에 그치는 것이 아니라 지금까지 그 경우의 수를 다 헤아릴 수 없을 것으로 알려져 있던 문제를 해결했다는 시사점을 갖는 것과 같다.  

<br>

수업에서도 언급된 바 있듯이 현재 인공지능을 이끌고 있는 알고리즘인 딥러닝을 있게 한 것은 크게 역전파를 이용한 알고리즘에서의 성능개선과 컴퓨팅 파워의 향상이라고 할 수 있다. 기존의 머신러닝의 경우 딥러닝에 비해 적은 양의 데이터에서도 좋은 성능을 보이지만, 일정수준 이상의 성능을 끌어내는 것이 어려웠다. 이에 반해 딥러닝은 데이터 양에 비례해 성능이 증가한다<sup>[[9]](#footnote_9)</sup>. 다만 딥러닝 알고리즘이 처음 등장했을 무렵엔 충분한 성능을 낼 만큼의 많은 데이터를 처리할 수 있는 컴퓨팅 파워가 뒷받침 되지 못했고, 딥러닝은 실패한 알고리즘으로 치부될 수밖에 없었다.  
기존의 중앙처리장치인 CPU(Central Processing Unit)는 순차적인 연산을 하는 장치이다. 이러한 형태의 장치엔 전자의 이동속도라는 필연적인 한계점이 존재한다. 도입부에 언급했듯이 하드웨어를 개발하는 회사들은 공정에서의 소형화를 통해 이 속도를 최대한 줄이고 있지만 그 비용에 비해 성능의 개선은 점점 더뎌질 수밖에 없다<sup>[[10]](#footnote_10)</sup>. 이에 성능 향상을 위한 또 하나의 축이 등장하는데 그게 바로 병렬화이다.  
병렬 컴퓨팅을 이용하게 되면 동시에 서로 독립적인 여러 연산을 처리할 수 있고, 이를 통해 연산속도의 향상을 끌어낼 수 있다. 예를 들면 A연산에는 a, b라는 변수가 이용되고, B연산에는 c, d라는 변수가 이용되며, 두 연산은 모두 3의 시간을 필요로 하다고 가정하자. A연산이 일어난 뒤에 B연산을 하면 6의 시간이 소요되지만 두 연산을 동시에 실행하면 그 속도가 절반으로 단축된다. 그리고 이러한 병렬연산을 가능케 한 대표적인 하드웨어로는 GPU(Graphics Processing Unit)를 들 수 있을 것이다.  
기존의 CPU는 PC당 하나의 장치로 연산을 했지만, GPU는 여러 개를 장착할 수 있기 때문에 그 개수를 늘리는 것으로 성능을 향상시킬 수 있다. 이는 딥러닝 관련 커뮤니티에서 최상급 GPU 하나보다 중상급 GPU 2개를 추천하는 것을 통해서도 살펴볼 수 있다. 가격이 2배 차이난다고 해서 성능이 2배 차이 나는 것은 아니기 때문에 개별의 성능을 높이는 것보다 개수를 늘리는 것이 비용을 줄이는 데에 도움이 된다.  
다만 병렬 컴퓨팅을 이용해 그 수를 늘리는 데에도 한계가 있다. 이러한 형태의 연산은 어디까지나 독립적인 연산에서만 유효하며, 필요 이상으로 늘리게 되면 낭비가 발생하기 때문이다. 비슷한 예를 들어보자면 A연산에 변수 a, b가 이용되고, B연산에 b, c가 이용된다면, 두 연산을 동시에 실행했을 때 b변수에서 충돌이 일어날 수 있기 때문에 병렬 연산을 이용할 수 없다. 만약 2개의 연산 장치가 제공되었다면 하나의 장치는 낭비되는 것이다. 따라서 하드웨어 자체에서의 성능 향상 또한 필수적이다. Google에서 발표한 두 개의 자연어 처리 알고리즘에 관한 모델인 Transformer<sup>[[11]](#footnote_11)</sup>와 BERT<sup>[[12]](#footnote_12)</sup>의 컴퓨팅 자원에 대한 직접적인 언급이 있는데, 이를 인용해 사례로 들어보려고 한다. Transformer에서는 NVIDIA의 Tesla P100을 8개 사용했고, BERT에서는 Large model 기준으로 16개의 Cloud TPU(64 TPU chip)를 사용했다. BERT 학습에 4일이 소요되었지만 만약 P100 8개를 이용했다면 1년 넘는 시간이 걸렸을 수도 있다고 밝혔다<sup>[[13]](#footnote_13)</sup>. 이를 통해서 성능이 향상된 알고리즘에는 그를 뒷받침할 하드웨어가 필요하다는 것을 알 수 있다.  
안타깝게도 관련 수업에서는 알고리즘에 초점을 맞추어 하드웨어적인 부분을 소홀히 하는 경향이 있다. 반면에 많은 프로젝트를 할수록 점점 하드웨어에 대한 집착이 강해지는 경향 또한 나타나는 것 같다. 물론 직접적인 결과를 내는 것은 알고리즘의 변화를 통해서라고 볼 수 있다. 하지만 전통적인 연산방식에서 병렬 컴퓨팅으로 넘어오면서 딥러닝이라는 알고리즘을 소화할 수 있게 된 것처럼 하드웨어 성능의 향상은 가용한 알고리즘의 스펙트럼을 넓혀줄 수 있다.  
인지과학과 직접적으로 연관되어있지 않다고 생각하여 사례에서는 언급하지 않았지만, 3진법 기반 반도체<sup>[[14]](#footnote_14)</sup>나 양자컴퓨터 등을 포함한 새로운 기술의 등장으로 기존의 모든 컴퓨터와 관련된 상식들이 바뀔 수 있다는 사실은 익히 들어왔을 것이다. Google은 양자컴퓨터<sup>[[15]](#footnote_15)</sup>를 이용해 슈퍼컴퓨터에서 1만년 걸리는 연산을 ‘단 53개의 소자로’ 200초에 실행했다고 발표하였고, 3진법 기반 반도체는 1bit로 제 3의 상태를 나타낼 수 있게 해줄 것이다. 그리고 사례로 들었던 뉴로모픽 칩 역시 새로운 감각을 컴퓨팅의 영역으로 끌어오며 그 범위를 넓혔다.
딥러닝으로 성능이 개선되었다고는 하지만 여전히 인공지능은 전문가 프로그램 수준에 머물러있다. 하지만 하드웨어 성능이 개선되며 지금껏 불가능했던 연산들이 가능해지면 언젠가 SF속에만 존재하던 강인공지능이 등장할 수 있을지도 모른다. 다만 이는 알고리즘을 실행할 수 있을 만큼의 하드웨어에서의 성능 개선이 뒷받침되어야 한다는 점은 자명하다.  

----

> ### Reference
> <a name="footnote_1">[1]</a> : 리사수님이 이제는 64코어라고 하셨다 [CES 2020] [웹사이트], (2020, January 6). URL: https://www.youtube.com/watch?v=QZU8H4d8KmA  
> <a name="footnote_2">[2]</a> : AI 천재 인텔과 최신형 10나노+ 타이거레이크 칩셋 [CES 2020] [웹사이트], (2020, January 7). URL: https://www.youtube.com/watch?v=d4ntM0wdpN4  
> <a name="footnote_3">[3]</a> : 코 없는 컴퓨터 칩이 냄새를 구별한다 '뉴로모픽 컴퓨팅' [웹사이트], (2020 March 19), URL: https://n.news.naver.com/article/293/0000027223  
> <a name="footnote_4">[4]</a> : 박종길 (2017), 뉴로모픽 소자의 현재와 미래, 진공 이야기 Vacuum Magazine  
> <a name="footnote_5">[5]</a> : Mike D., Narayan S., T. H. Lin, Gautham C., Yongqiang C., Sri H. C., ... , H. Wang. (2018), Loihi: A Neuromophic Manycore Processor with On-Chip Learning, IEEE Micro January/February 2018  
> <a name="footnote_6">[6]</a> : 선형 회귀(linear regression) 그리고 라쏘(Lasso) [웹사이트], (2017 November 10), URL: https://bskyvision.com/193  
> <a name="footnote_7">[7]</a> : Intel's Neuromorphic Chip Scales Up (and It Smells) [Website], (2020 March 18), URL: https://www.hpcwire.com/2020/03/18/intels-neuromorphic-chip-scales-up-and-it-smells/  
> <a name="footnote_8">[8]</a> : 인텔, 냄새 맡는 컴퓨터 칩 개발 [웹사이트], (2020 March 19), URL: http://naver.me/Gs3EICPY  
> <a name="footnote_9">[9]</a> : 1. 데이터 어그멘테이션 연구 동향을 소개합니다. [웹사이트], (2019 June 20), URL: https://www.kakaobrain.com/blog/64  
> ![image](https://user-images.githubusercontent.com/39390943/92552947-c3adf900-f29c-11ea-9930-a9ed874aceb6.png)  
> <a name="footnote_10">[10]</a> : 암달의 법칙 [웹사이트], (2018 September 13), URL: https://ko.wikipedia.org/wiki/암달의_법칙  
> <a name="footnote_11">[11]</a> : Ashish V., Noam S., Niki P., Jakob U., Llion J., Aidan N. G., ..., Illia P. (2017), Attention Is All You Need, 31st Conference on Neural Information Processing Systems (NIPS 2017),   
> <a name="footnote_12">[12]</a> : Jacob D., M. Chang, K. Lee, Kristina T. (2018), BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding  
> <a name="footnote_13">[13]</a> : 최첨단 인공지능 솔루션들 : (1) 구글 BERT, 인간보다 언어를 더 잘 이해하는 AI 모델 [웹사이트], (2018 November 12], URL: https://medium.com/ai-networkkr/최첨단-인공지능-솔루션들-1-구글-bert-인간보다-언어를-더-잘-이해하는-ai-모델-9704ebc016c4  
> <a name="footnote_14">[14]</a> : [과학동아 기획기사] 3진법 소자, 하드웨어 인공지능을 꿈꾼다 [웹사이트], (2017 January 18), URL: http://ece.unist.ac.kr/kor/krkim_170111/  
> <a name="footnote_15">[15]</a> : [초점] 구글 양자컴퓨터의 진짜 위력은? [웹사이트], (2019 December 31), URL: http://www.aitimes.kr/news/articleView.html?idxno=15037  
